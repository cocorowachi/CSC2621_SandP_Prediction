{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726d26ef-470e-4997-83d4-b4a3d61c9019",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1334436676.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    Liam Otten, Cocoro Wachi\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Final Project: Stonks Go Up\n",
    "Liam Otten, Cocoro Wachi\n",
    "\n",
    "---\n",
    "## Deliverables (copied from below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1647c48b-0b12-422a-a457-45ef950b6dc7",
   "metadata": {},
   "source": [
    "1. Research Question\n",
    "    - Can we predict S&P 500 price over the next 10 days using historical data of common market indicators?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58893447-ac5b-4db1-86bb-c6c23a72ee7d",
   "metadata": {},
   "source": [
    "2. Hypothesis\n",
    "    - Null: There is no correlation between, common market indicators such as Dow Jones index, US GDP, Consumer Price Index, Home Price Index, and Foreign Exchange rates, Federal Bond Yield, and S&P price such that we can predict future S&P 500 price with a linear regression.\n",
    "    - Alternative: There is correlation between, common market indicators such as Dow Jones index, US GDP, CPI, HPI, and Foreign Exchange rates, Federal Bond Yield, and S&P price such that we can predict future S&P 500 price with a linear regressio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b796a0-3e34-47cd-90a7-9aa64e164fe0",
   "metadata": {},
   "source": [
    "3. Dataset\n",
    "   - Each column(feature) was collected independently from websites like investing.com and yahoo-finance\n",
    "   - Then the features were combined manually with standardized date time\n",
    "   - Some features are given an extra feature that is the natural log of the value, since they originally grow exponentially, and we want a linear relationship to fit a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426894b-0bd2-4bd9-801c-26654ce0931e",
   "metadata": {},
   "source": [
    "4. Data Preprocessing\n",
    "   - Since we had a variety of data collection (daily, weekly, monthly and quarterly), we used ffill() to fill missing values.\n",
    "   - We removed weekends since the market is closed and could affect the movement of the model/prediction.\n",
    "   - After looking at the dataset, we decided to use data after 1987, since that was when all features were present\n",
    "   - Percentage of Nan in each column is listed below.\n",
    "   - ~~calculated centered-moving average for S&P price to see a broader trend~~\n",
    "   - Engineered time series features so that there are columns for daily features -1 days, -2 days, etc up to 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ea1e4-5c5d-4001-8b72-ad3df04225f0",
   "metadata": {},
   "source": [
    "5. Data Analysis\n",
    "    - We implemented a 2 level linear regression model.\n",
    "    - Level 1 fits all features and selects which features have the highest correlation.\n",
    "    - Level 2 uses those high-correlated features to make a more accurate prediction.\n",
    "        - Values are first normalized for equal weights between the features.\n",
    "        - We used adjusted R^2 as our optimization function, then use that to find which features have the highest R^2\n",
    "        - Select the features (Greedy model). For each features, the model only keeps it if the addition of the feature improves the R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869b4cc-2e27-444e-a25a-d2d48b686db8",
   "metadata": {},
   "source": [
    "6. Modeling, Testing and Visualization\n",
    "   - Below is a method that tests a specific date and pprojects the prediction onto a graph along with the actual data\n",
    "   - The method creates a price prediction for each of the prediction range (day 1 to 15), and graphs it to show a continous prediction\n",
    "   - The \"Idea one\" is shown a few cells down, which predicts a vector of the next 10 days.\n",
    "   - We use sklearn for regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c74f0-c2b0-405f-8192-12d2a73896ce",
   "metadata": {},
   "source": [
    "7. Results Analysis\n",
    "   - The result of \"Idea 2\" (which appears first in this notebook) accurately captures the broad movement of the 15 day predictive span.\n",
    "   - Metrics that we used weren't really applicable, as we would have to make executive decisions about what days or predictions to actually plug into the metrics, and these would fail to represent our technique as a whole. Generally the graphs are the best indicator, and it indicates that our techniques are unpredictable and not very good.\n",
    "   - The measure we tried to implement for each predictive events: MSE and RMSE doesn't accurately measure the accuracy of the model, since the absolute value of the price changes, and when the price is high, MSE goes up even if the prediction is more accurate.\n",
    "   - For the future, we are looking into taking more historical data into our model, while keeping the number of features on the conservative side: coming up with a way to select which features are effective in the predictive model.\n",
    "   - We would also like to try other types of models, such as neural networks, and maybe look at percent change instead of price as a whole. These might be better for our non-linear relationships, and the model should likely do a better job at predicting the variances.\n",
    "   - Another point is expanding the type of indicators. There are numerous financial indicators specifically made for S&P price, so we want to explore those in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453714be-ec84-4814-8185-41dffed78e55",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2093a0b-d537-459d-8e00-68e8dc301b41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff7311-b724-4989-9e88-093783e68e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_yield_df = pd.read_csv(\"Bond yield.csv\")\n",
    "cpi_df = pd.read_csv(\"Consumer price index.csv\")\n",
    "dj_df = pd.read_csv(\"Dow Jones.csv\")\n",
    "usd_eur_df = pd.read_csv(\"USD_EUR.csv\")\n",
    "usd_jpy_df = pd.read_csv(\"USD_JPY.csv\")\n",
    "gdp_df = pd.read_csv(\"GDP.csv\")\n",
    "national_hpi_df = pd.read_csv(\"National home price index.csv\")\n",
    "sp_df = pd.read_csv(\"S&P Price.csv\")\n",
    "\n",
    "dfs = [bond_yield_df, cpi_df, dj_df, usd_eur_df, usd_jpy_df, gdp_df, national_hpi_df, sp_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5350a3-0d07-4357-95b0-3200441918c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    dfs[i].columns.values[0] = 'date'\n",
    "    dfs[i] = dfs[i].drop_duplicates(subset=\"date\", keep=\"first\") \n",
    "    dfs[i].set_index('date', inplace=True)\n",
    "    dfs[i].index = pd.to_datetime(dfs[i].index, errors=\"coerce\").normalize()\n",
    "    dfs[i] = dfs[i][~dfs[i].index.duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae4c03-fc23-480a-8a88-5494f476d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b9863-e6b0-4ba9-ab56-63ce5ff4a97d",
   "metadata": {},
   "source": [
    "4. Data Preprocessing\n",
    "   - Since we had a variety of data collection (daily, weekly, monthly and quarterly), we used ffill() to fill missing values.\n",
    "   - We removed weekends since the market is closed and could affect the movement of the model/prediction.\n",
    "   - After looking at the dataset, we decided to use data after 1987, since that was when all features were present\n",
    "   - Percentage of Nan in each column is listed below.\n",
    "   - Calculated centered-moving average for S&P price to see a broader trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58aa459-f528-45bf-80bf-14b2dae7b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df[['Value', 'DJIA', 'GDP', 'DGS10', 'CSUSHPINSA', 'CPIAUCSL', 'EURO', 'JPY']]\n",
    "df.columns = ['SP_price','dow_jones','gdp','bond_yield','home_price_index','consumer_price_index','eur','jpy']\n",
    "df.reset_index(inplace=True)\n",
    "full_date_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')\n",
    "df.set_index(\"date\", inplace=True) \n",
    "df = df.reindex(full_date_range)\n",
    "df.index.name = \"date\"\n",
    "nan_counts = df.isna().mean() * 100\n",
    "print('Nan value Percentage for each column')\n",
    "print(nan_counts)\n",
    "df= df.ffill()\n",
    "\n",
    "df = df[df.index >= '1987-01-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7d231-8a19-4400-8fb2-c749278bcc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mirrored_rows(df, num_rows=30):\n",
    "    \"\"\"\n",
    "    Insert chronologically mirrored data point at head and tail of df\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    mirrored_rows_head = df.iloc[:num_rows].copy()\n",
    "    mirrored_rows_head = mirrored_rows_head.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    mirrored_rows_tail = df.iloc[-num_rows:].copy()\n",
    "    mirrored_rows_tail = mirrored_rows_tail.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    df_extended = pd.concat([mirrored_rows_head, df, mirrored_rows_tail], ignore_index=True)\n",
    "    \n",
    "    return df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2b64f-ae64-4276-b3ac-f17a1c1443b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = insert_mirrored_rows(df['SP_price'].copy(), 200)\n",
    "ma30_df = df.rolling(window=30).mean().reset_index().iloc[30:-30].reset_index(drop=True)\n",
    "ma100_df = df.rolling(window=100).mean().reset_index().iloc[100:-100].reset_index(drop=True)\n",
    "ma200_df = df.rolling(window=200).mean().reset_index().iloc[200:-200].reset_index(drop=True)\n",
    "df['SP_MA_30'] = df['SP_price'].rolling(window=30).mean()\n",
    "df['SP_MA_100'] = df['SP_price'].rolling(window=100).mean()\n",
    "df['SP_MA_200'] = df['SP_price'].rolling(window=200).mean()\n",
    "\n",
    "df['SP_price_ln'] = np.log(df['SP_price'])\n",
    "df['dow_jones_ln'] = np.log(df['dow_jones'])\n",
    "df['gdp_ln'] = np.log(df['gdp'])\n",
    "df['home_price_index_ln'] = np.log(df['home_price_index'])\n",
    "df['bond_yield_ln'] = np.log(df['bond_yield'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f2f4d-217d-46fd-8230-1539c27cc645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d37d04-63e5-4f1a-84a4-01f633d4aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(200, 6))\n",
    "plt.plot(df['date'], df['SP_price'])\n",
    "plt.title('S&P 500 Price (1930–2020)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84297926-9ba1-4b71-a92c-aec537ccd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(200, 6))\n",
    "plt.plot(df['date'], df['SP_price_ln'])\n",
    "plt.title('S&P 500 Price (1930–2020)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc5c95-4c77-4c88-8d9f-2b02bffd058b",
   "metadata": {},
   "source": [
    "### Features we used are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66005152-7192-4cd6-8648-dd6a8044754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_features = [\"SP_price_ln\", \"dow_jones_ln\", \"gdp_ln\", \"eur\", \"jpy\", \"home_price_index_ln\", \"consumer_price_index\", \"bond_yield_ln\"]\n",
    "\n",
    "# Change predictive range from here\n",
    "#################\n",
    "lookback = 60\n",
    "forecast = 10\n",
    "#################\n",
    "new_cols = {}\n",
    "\n",
    "for f in daily_features:\n",
    "    new_cols[f + f\"+{forecast}\"] = df[f].shift(-forecast)\n",
    "\n",
    "for f in daily_features:\n",
    "    for i in range(1, lookback + 1):\n",
    "        new_cols[f + f\"-{i}\"] = df[f].shift(i)\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n",
    "\n",
    "df_with_wknd = df.copy()\n",
    "df = df[df['date'].dt.weekday < 5].reset_index(drop=True)\n",
    "# i = 1\n",
    "# for col in df.columns:\n",
    "#     print(i, col)\n",
    "#     i+=1\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bde404-10ef-4dad-9ae1-34a3d0ddc2e2",
   "metadata": {},
   "source": [
    "5. Data Analysis\n",
    "    - We implemented a 2 level linear regression model.\n",
    "    - Level 1 fits all features and selects which features have the highest correlation.\n",
    "    - Level 2 uses those high-correlated features to make a more accurate prediction.\n",
    "        - Values are first normalized for equal weights between the features.\n",
    "        - We used adjusted R^2 as our optimization function, then use that to find which features have the highest R^2\n",
    "        - Select the features (Greedy model). For each features, the model only keeps it if the addition of the feature improves the R^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efd91c-8769-4ffa-a00b-3fc9a2e800ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 1\n",
    "df_filtered = df[(df['date'].dt.year >= 1980) & (df['date'].dt.year <= 2020)]\n",
    "col =  df.iloc[:, 26:].columns\n",
    "df_filtered = df_filtered.dropna(subset=col)\n",
    "X = df_filtered[col]\n",
    "y = df_filtered[f'SP_price_ln+{forecast}']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4082c78-3962-41c8-b080-4575aa1d5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined = []\n",
    "for i in range(len(col)):\n",
    "    if abs(model.coef_[i]) > 0.1:\n",
    "        refined.append(col[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f57a17-422b-461a-bb2b-fbc163742a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36f3bd-0fdc-4626-815d-e3aa2d2bcd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model2 = df[(df['date'].dt.year >= 1980) & (df['date'].dt.year <= 2020)]\n",
    "features = df.iloc[:, 26:]\n",
    "\n",
    "scaler_global = StandardScaler()\n",
    "scaled_array = scaler_global.fit_transform(features)\n",
    "normalized_all = pd.DataFrame(scaled_array, index=features.index, columns=features.columns)\n",
    "\n",
    "n = len(normalized_all)\n",
    "\n",
    "def adj_r2(R2, p):\n",
    "    return 1 - (1 - R2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Your target columns\n",
    "target_columns = [\n",
    "    \"SP_price_ln\",\n",
    "    # \"dow_jones_ln\",\n",
    "    \"eur\",\n",
    "    \"jpy\",\n",
    "    # \"consumer_price_index\",\n",
    "    \"bond_yield_ln\"\n",
    "    # \"gdp_ln\"\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for y_col in target_columns:\n",
    "    print(f\"\\n=== Building model for: {y_col} ===\")\n",
    "    y = df_model2[y_col]\n",
    "    normalized = normalized_all.copy()\n",
    "    normalized[y_col] = y\n",
    "\n",
    "    mod_scores = []\n",
    "    for c in normalized.columns:\n",
    "        if c == y_col:\n",
    "            continue\n",
    "        model = LinearRegression(fit_intercept=True)\n",
    "        working_df = normalized.dropna(subset=[y_col, c])\n",
    "        col = working_df[c].to_numpy().reshape(-1, 1)\n",
    "        model.fit(col, working_df[y_col])\n",
    "        score = adj_r2(model.score(col, working_df[y_col]), 1)\n",
    "        mod_scores.append((c, score))\n",
    "\n",
    "    mod_scores = sorted(mod_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_score = 0\n",
    "    selected_cols = []\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "\n",
    "    for c, _ in mod_scores:\n",
    "        working_df = normalized.dropna(subset=selected_cols + [c, y_col])\n",
    "        X = working_df[selected_cols + [c]]\n",
    "        reg.fit(X, working_df[y_col])\n",
    "        score = adj_r2(reg.score(X, working_df[y_col]), len(selected_cols) + 1)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            selected_cols += [c]\n",
    "\n",
    "    # Final fit with scaler\n",
    "    final_scaler = StandardScaler()\n",
    "    working_df = df_model2.dropna(subset=selected_cols + [y_col])\n",
    "    X_raw = working_df[selected_cols]\n",
    "    X_scaled = pd.DataFrame(final_scaler.fit_transform(X_raw), columns=selected_cols)\n",
    "    X_scaled.index = working_df.index\n",
    "    reg.fit(X_scaled, working_df[y_col])\n",
    "\n",
    "    # Statsmodels OLS summary\n",
    "    X_scaled_with_const = sm.add_constant(X_scaled)\n",
    "    sm_model = sm.OLS(working_df[y_col], X_scaled_with_const).fit()\n",
    "    print(sm_model.summary())\n",
    "\n",
    "    # Save everything\n",
    "    models[y_col] = {\n",
    "        'features': selected_cols,\n",
    "        'scaler': final_scaler,\n",
    "        'reg': reg,\n",
    "        'sm_model': sm_model\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad40cdd-aa6e-4d1f-8eca-047bde441fe3",
   "metadata": {},
   "source": [
    "6. Modeling, Testing and Visualization\n",
    "   - Below is a method that tests a specific date and pprojects the prediction onto a graph along with the actual data\n",
    "   - The method creates a price prediction for each of the prediction range (day 1 to 15), and graphs it to show a continous prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f6b32-13bf-42b1-a382-07c54c98331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(selected_date, window=30):\n",
    "    selected_date = pd.to_datetime(selected_date)\n",
    "    selected_idx = df[df['date'] == selected_date].index[0]\n",
    "\n",
    "    predicted_series = {target: [] for target in models.keys()}\n",
    "    predicted_dates = []\n",
    "\n",
    "    current_row = df.iloc[selected_idx].copy()\n",
    "\n",
    "    for step in range(forecast):\n",
    "        idx = selected_idx + step\n",
    "        predicted_dates.append(df['date'].iloc[idx])\n",
    "\n",
    "        # Iterate over each model to predict and update\n",
    "        for target_var, model_data in models.items():\n",
    "            features = model_data['features']\n",
    "            scaler = model_data['scaler']\n",
    "            reg = model_data['reg']\n",
    "\n",
    "            # Prepare features for prediction\n",
    "            X_features = current_row[features]\n",
    "            X_scaled = pd.DataFrame(scaler.transform(X_features.to_frame().T), columns=features)\n",
    "\n",
    "            pred_ln = reg.predict(X_scaled)[0]\n",
    "            predicted_series[target_var].append(pred_ln)\n",
    "\n",
    "            # Find lagged columns for this target (e.g., SP_price_ln_t-1)\n",
    "            lagged_cols = [col for col in features if target_var in col]\n",
    "            if lagged_cols:\n",
    "                shifted_vals = current_row[lagged_cols].copy()\n",
    "                shifted_vals = [pred_ln] + list(shifted_vals.values[:-1])\n",
    "                current_row[lagged_cols] = shifted_vals\n",
    "\n",
    "        # No need to handle valid_indices if dates are aligned\n",
    "\n",
    "    # Plot SP500 as the main example\n",
    "    plot_start = selected_idx - window\n",
    "    plot_end = selected_idx + forecast + window\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(df.index[plot_start:plot_end], df['SP_price'].iloc[plot_start:plot_end],\n",
    "             label='Actual SP500', linewidth=4, color='blue')\n",
    "    plt.plot(range(selected_idx, selected_idx + forecast),\n",
    "             np.exp(predicted_series['SP_price_ln']), 'ro--',\n",
    "             label=f'Iterative SP500 prediction ({forecast} days)', linewidth=3, markersize=8)\n",
    "\n",
    "    plt.axvline(x=selected_idx, color='green', linewidth=3)\n",
    "\n",
    "    xticks = df.index[plot_start:plot_end:5]\n",
    "    xlabels = df['date'].iloc[plot_start:plot_end:5].dt.strftime('%Y-%m-%d')\n",
    "    plt.xticks(ticks=xticks, labels=xlabels, rotation=45)\n",
    "\n",
    "    plt.title(f\"Iterative {forecast}-day prediction from {selected_date.date()}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('S&P 500 Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Error metric for SP500\n",
    "    actual_prices = df['SP_price'].iloc[selected_idx:selected_idx + forecast].values\n",
    "    predicted_prices = np.array(predicted_series['SP_price_ln']).flatten()\n",
    "\n",
    "    mse = mean_squared_error(actual_prices, predicted_prices)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "    # Optional: print other series' last predicted values\n",
    "    for target_var, preds in predicted_series.items():\n",
    "        print(f\"Last predicted {target_var}: {preds[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555773e-76ad-49aa-8f13-6d35977472bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('2023-8-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7a7c6-a970-4d64-b402-0d213f56ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('2022-11-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9db4a-0384-49e8-8362-1be5312ae342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('2023-5-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9899fe2-3d41-4367-be0b-7ad5bdf52c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('2024-11-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65737fa0-1263-4273-9b7f-ee188504d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('2022-5-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37099d88-5bf3-41d8-969b-42d3206b2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('2022-7-29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeddbf7-eee9-4d1b-aa81-64bc5ac33574",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions('2022-8-17')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70387710-bb4a-4638-bd43-e976172cc7af",
   "metadata": {},
   "source": [
    "7. Results Analysis\n",
    "   - The result accurately captures the broad movement of the 15 day predictive span.\n",
    "   - Looking at the OLS regression results, the R^2 is extremely high, and Probablity of F-stats is extremely low, indicating that the model is accurate when looked at whole. We concluded to reject H0, and state that the linear regression model is accurate.\n",
    "   - The measure we tried to implement for each predictive events: MSE and RMSE doesn't accurately measure the accuracy of the model, since the absolute value of the price changes, and when the price is high, MSE goes up even if the prediction is more accurate.\n",
    "   - For the future, we are looking into taking more historical data into our model, while keeping the number of features on the conservative side: coming up with a way to select which features are effective in the predictive model.\n",
    "   - Another point is expanding the type of indicators. There are numerous financial indicators specifically made for S&P price, so we want to explore those in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea1d2a-a813-4034-9d82-51311918eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = 10\n",
    "def plot_all_predictions(selected_date, window=15):\n",
    "    selected_date = pd.to_datetime(selected_date)\n",
    "    selected_idx = df[df['date'] == selected_date].index[0]\n",
    "\n",
    "    # Store predictions + confidence intervals\n",
    "    predicted_series = {\n",
    "        target: {'preds': [], 'lower': [], 'upper': []}\n",
    "        for target in models.keys()\n",
    "    }\n",
    "    predicted_dates = []\n",
    "\n",
    "    current_row = df.iloc[selected_idx].copy()\n",
    "\n",
    "    for step in range(forecast):\n",
    "        idx = selected_idx + step\n",
    "        predicted_dates.append(df['date'].iloc[idx])\n",
    "\n",
    "        # Predict each target and update lagged features\n",
    "        for target_var, model_data in models.items():\n",
    "            features = model_data['features']\n",
    "            scaler = model_data['scaler']\n",
    "            reg = model_data['reg']\n",
    "            sm_model = model_data['sm_model']\n",
    "\n",
    "            # Prepare features for prediction\n",
    "            X_features = current_row[features]\n",
    "            X_scaled = pd.DataFrame(scaler.transform(X_features.to_frame().T), columns=features)\n",
    "\n",
    "            # Match exact columns from training (avoid constant mismatch)\n",
    "            X_scaled_with_const = pd.DataFrame(\n",
    "                sm.add_constant(X_scaled, has_constant='add'),\n",
    "                columns=sm_model.model.exog_names\n",
    "            )\n",
    "\n",
    "            pred = sm_model.get_prediction(X_scaled_with_const)\n",
    "            pred_summary = pred.summary_frame(alpha=0.05)\n",
    "\n",
    "            pred_value = pred_summary['mean'].iloc[0]\n",
    "            lower_bound = pred_summary['obs_ci_lower'].iloc[0]\n",
    "            upper_bound = pred_summary['obs_ci_upper'].iloc[0]\n",
    "\n",
    "            # Store\n",
    "            predicted_series[target_var]['preds'].append(pred_value)\n",
    "            predicted_series[target_var]['lower'].append(lower_bound)\n",
    "            predicted_series[target_var]['upper'].append(upper_bound)\n",
    "\n",
    "            # Update lagged columns if applicable\n",
    "            lagged_cols = [col for col in features if target_var in col]\n",
    "            if lagged_cols:\n",
    "                shifted_vals = current_row[lagged_cols].copy()\n",
    "                shifted_vals = [pred_value] + list(shifted_vals.values[:-1])\n",
    "                current_row[lagged_cols] = shifted_vals\n",
    "\n",
    "    # Plotting\n",
    "    num_targets = len(models)\n",
    "    fig, axes = plt.subplots(num_targets, 1, figsize=(10, 4 * num_targets), sharex=True)\n",
    "\n",
    "    if num_targets == 1:\n",
    "        axes = [axes]  # Make iterable if only one\n",
    "\n",
    "    plot_start = selected_idx - 35\n",
    "    plot_end = selected_idx + forecast + 10\n",
    "    x_range = df.index[plot_start:plot_end]\n",
    "    xticks = df.index[plot_start:plot_end:5]\n",
    "    xlabels = df['date'].iloc[plot_start:plot_end:5].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    for ax, (target_var, series_data) in zip(axes, predicted_series.items()):\n",
    "        # Clean name (remove '_ln')\n",
    "        clean_name = target_var.replace('_ln', '')\n",
    "\n",
    "        # Actual series (possibly exp)\n",
    "        actual_series = df[target_var].iloc[plot_start:plot_end]\n",
    "        if target_var.endswith('_ln'):\n",
    "            actual_series = np.exp(actual_series)\n",
    "            preds = np.exp(series_data['preds'])\n",
    "            lower = np.exp(series_data['lower'])\n",
    "            upper = np.exp(series_data['upper'])\n",
    "        else:\n",
    "            preds = series_data['preds']\n",
    "            lower = series_data['lower']\n",
    "            upper = series_data['upper']\n",
    "\n",
    "        forecast_idx_range = range(selected_idx, selected_idx + forecast)\n",
    "\n",
    "        ax.plot(x_range, actual_series,\n",
    "                label=f'Actual {clean_name}', linewidth=3, color='blue')\n",
    "\n",
    "        ax.plot(forecast_idx_range, preds,\n",
    "                'ro--', label=f'Predicted {clean_name}', linewidth=3, markersize=6)\n",
    "\n",
    "        # Add confidence interval as shaded area\n",
    "        # ax.fill_between(forecast_idx_range, lower, upper,\n",
    "        #                 color='gray', alpha=0.4, label='95% Prediction Interval')\n",
    "\n",
    "        ax.axvline(x=selected_idx-1, color='green', linewidth=3)\n",
    "        ax.set_title(f'{clean_name} prediction')\n",
    "        ax.legend()\n",
    "        \n",
    "        ax.axvspan(idx-30, idx-1, color=\"gray\", alpha=0.3)\n",
    "\n",
    "    # Set shared x-ticks & labels at the end (global)\n",
    "    plt.xticks(ticks=xticks, labels=xlabels, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea6b3e-5af5-4a2b-b499-53f6862a3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_predictions('2023-4-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6aed2-508f-4c5c-a213-de6f11aad78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_all_predictions('2022-11-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67587561-484f-4abf-a886-e13c7ff3dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_predictions('2022-10-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f24ce-7f87-423e-a9e5-763c3d770355",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_predictions('2022-11-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88de5d2-bf26-4f51-ad74-a46447f25c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pretty good\n",
    "plot_all_predictions('2025-2-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d0399-e486-45d5-9348-fd38f31e70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_predictions('2024-2-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f730808-32a0-44e8-9452-ed3e66a19d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## good example for bad fitting\n",
    "plot_all_predictions('2023-8-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397255f9-81ed-492b-b6ba-7424bb388683",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### good example for good modeling\n",
    "plot_all_predictions('2022-7-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a52ab-bf63-49ae-8cd5-e5f2844688f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 3\n",
    "new_cols = {}\n",
    "df_model3 = df[(df['date'].dt.year >= 1980) & (df['date'].dt.year <= 2020)]\n",
    "for i in range(forecast):\n",
    "    new_cols[f'SP_price_ln+{i}'] = df_model3['SP_price_ln'].shift(-i)\n",
    "df_model3 = pd.concat([df_model3, pd.DataFrame(new_cols)], axis=1)\n",
    "\n",
    "normalized = df_model3.copy()\n",
    "scaler = StandardScaler()\n",
    "y_col = [f'SP_price_ln+{i}' for i in range(forecast)]\n",
    "\n",
    "y = df_model3[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd6909-365b-4213-82de-160c8ecd3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_array = scaler.fit_transform(features)\n",
    "normalized = pd.DataFrame(scaled_array, index=features.index, columns=features.columns)\n",
    "normalized[y_col] = y\n",
    "\n",
    "n = len(normalized)\n",
    "def adj_r2(R2, p):\n",
    "    return 1 - (1-R2) * (n-1) / (n-p-1)\n",
    "\n",
    "mod_3_scores = []\n",
    "for c in normalized.columns:\n",
    "    if c in y_col: continue\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    working_df = normalized.dropna(subset=y_col + [c])\n",
    "    col = working_df[c].to_numpy().reshape(-1, 1)\n",
    "    model.fit(col, working_df[y_col])\n",
    "    score = adj_r2(model.score(col, working_df[y_col]), 1)\n",
    "    mod_3_scores.append((c, score))\n",
    "\n",
    "mod_3_scores = sorted(mod_3_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "mod_3_score = 0\n",
    "mod_3_cols = []\n",
    "reg3 = LinearRegression(fit_intercept=True)\n",
    "for c, _ in mod_3_scores:\n",
    "    working_df = normalized.dropna(subset=mod_3_cols + y_col + [c])\n",
    "    X = working_df[mod_3_cols + [c]]\n",
    "    reg3.fit(X, working_df[y_col])\n",
    "    score = adj_r2(reg3.score(X, working_df[y_col]), len(mod_3_cols) + 1)\n",
    "    if score > mod_3_score:\n",
    "        mod_3_score = score\n",
    "        mod_3_cols += [c]\n",
    "\n",
    "final_scaler = StandardScaler()\n",
    "working_df = df_model3.dropna(subset=mod_3_cols + y_col)\n",
    "X3_raw = working_df[mod_3_cols]\n",
    "X3_scaled = pd.DataFrame(final_scaler.fit_transform(X3_raw), columns=mod_3_cols)\n",
    "X3_scaled.index = working_df.index\n",
    "\n",
    "reg3.fit(X3_scaled, working_df[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dba9e1-60db-4e8e-9e23-98fe23f09ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions2(selected_date, window=30):\n",
    "    selected_date = pd.to_datetime(selected_date)\n",
    "    selected_idx = df[df['date'] == selected_date].index[0]\n",
    "\n",
    "    # Extract current feature row and scale it\n",
    "    current_row = df.iloc[selected_idx]\n",
    "    X_features = current_row[mod_3_cols].to_frame().T  # shape: (1, n_features)\n",
    "    X_scaled = pd.DataFrame(final_scaler.transform(X_features), columns=mod_3_cols)\n",
    "\n",
    "    # Predict the entire window of log prices\n",
    "    pred_ln_vector = reg3.predict(X_scaled)[0]\n",
    "    predicted_prices = np.exp(pred_ln_vector)  # Convert log-prices to prices\n",
    "\n",
    "    # Determine the corresponding future dates\n",
    "    predicted_dates = df['date'].iloc[selected_idx:selected_idx + forecast].values\n",
    "\n",
    "    # Plotting\n",
    "    plot_start = selected_idx - window\n",
    "    plot_end = selected_idx + forecast + window\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    plt.plot(df.index[plot_start:plot_end], df['SP_price'].iloc[plot_start:plot_end],\n",
    "             label='Actual price', linewidth=4, color='blue')\n",
    "    plt.plot(range(selected_idx, selected_idx + forecast), predicted_prices,\n",
    "             'ro--', label=f'Vector prediction ({forecast} days)', linewidth=3, markersize=8)\n",
    "    plt.axvline(x=selected_idx, color='green', linewidth=3)\n",
    "\n",
    "    xticks = df.index[plot_start:plot_end:5]\n",
    "    xlabels = df['date'].iloc[plot_start:plot_end:5].dt.strftime('%Y-%m-%d')\n",
    "    plt.xticks(ticks=xticks, labels=xlabels, rotation=45)\n",
    "    \n",
    "    plt.title(f\"Vector {forecast}-day prediction from {selected_date.date()}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('S&P 500 Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Error metrics if possible\n",
    "    actual_prices = df['SP_price'].iloc[selected_idx:selected_idx + forecast].values\n",
    "    mse = mean_squared_error(actual_prices, predicted_prices)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Mean Squared Error: {mse:.4f}')\n",
    "    print(f'Root Mean Squared Error: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f1fa6-f80e-486f-b507-378a7915c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions2('2023-8-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d8ef6-0f2b-4af3-8a3e-901a961815e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions2('2025-2-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6172d-749a-4aee-8213-ace4309749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions2('2024-6-28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67f68a-40b0-482c-801c-b0fef90f5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions2('2024-11-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
